{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from domain.text_vectorizers import ASCIIVectorizer, BoWVectorizer, BiLSTMVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/spam_classify/train.csv\")\n",
    "test = pd.read_csv(\"data/spam_classify/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = list(train.v2), list(train.v1)\n",
    "test_texts, test_labels = list(test.v2), list(test.v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agg method: mean\n",
      "Training accuracy: 0.537, Precison: 0.537, Recall: 0.538, F1 score: 0.538\n",
      "Test accuracy: 0.514, Precison: 0.514, Recall: 0.515, F1 score: 0.514\n",
      "\n",
      "Agg method: first\n",
      "Training accuracy: 0.537, Precison: 0.540, Recall: 0.494, F1 score: 0.516\n",
      "Test accuracy: 0.512, Precison: 0.513, Recall: 0.474, F1 score: 0.493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "for method in [\"mean\", \"first\"]:\n",
    "    print(f\"Agg method: {method}\")\n",
    "    vectorizer = ASCIIVectorizer(max_features=128, char_aggregator=method) \n",
    "    \n",
    "    X_train, y_train = vectorizer.transform(train_texts, train_labels)\n",
    "    X_test, y_test = vectorizer.transform(test_texts, test_labels)\n",
    "    \n",
    "    # Fit the model to training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_prec = precision_score(y_train, y_train_pred)\n",
    "    train_rec = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec = precision_score(y_test, y_test_pred)\n",
    "    test_rec = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_acc:.3f}, \"\n",
    "          f\"Precison: {train_prec:.3f}, \"\n",
    "          f\"Recall: {train_rec:.3f}, \"\n",
    "          f\"F1 score: {train_f1:.3f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.3f}, \"\n",
    "          f\"Precison: {test_prec:.3f}, \"\n",
    "          f\"Recall: {test_rec:.3f}, \"\n",
    "          f\"F1 score: {test_f1:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8\n",
      "Training accuracy: 0.864, Precison: 0.441, Recall: 0.025, F1 score: 0.047\n",
      "Test accuracy: 0.873, Precison: 1.000, Recall: 0.028, F1 score: 0.054\n",
      "\n",
      "Vocab size: 16\n",
      "Training accuracy: 0.897, Precison: 0.738, Recall: 0.374, F1 score: 0.496\n",
      "Test accuracy: 0.905, Precison: 0.753, Recall: 0.400, F1 score: 0.523\n",
      "\n",
      "Vocab size: 32\n",
      "Training accuracy: 0.919, Precison: 0.774, Recall: 0.563, F1 score: 0.652\n",
      "Test accuracy: 0.922, Precison: 0.796, Recall: 0.538, F1 score: 0.642\n",
      "\n",
      "Vocab size: 64\n",
      "Training accuracy: 0.939, Precison: 0.849, Recall: 0.664, F1 score: 0.746\n",
      "Test accuracy: 0.935, Precison: 0.841, Recall: 0.621, F1 score: 0.714\n",
      "\n",
      "Vocab size: 128\n",
      "Training accuracy: 0.969, Precison: 0.946, Recall: 0.816, F1 score: 0.876\n",
      "Test accuracy: 0.956, Precison: 0.875, Recall: 0.772, F1 score: 0.821\n",
      "\n",
      "Vocab size: 256\n",
      "Training accuracy: 0.982, Precison: 0.980, Recall: 0.882, F1 score: 0.928\n",
      "Test accuracy: 0.973, Precison: 0.946, Recall: 0.841, F1 score: 0.891\n",
      "\n",
      "Vocab size: 512\n",
      "Training accuracy: 0.985, Precison: 0.993, Recall: 0.897, F1 score: 0.942\n",
      "Test accuracy: 0.978, Precison: 0.976, Recall: 0.848, F1 score: 0.908\n",
      "\n",
      "Vocab size: 1024\n",
      "Training accuracy: 0.990, Precison: 0.998, Recall: 0.927, F1 score: 0.961\n",
      "Test accuracy: 0.979, Precison: 0.977, Recall: 0.862, F1 score: 0.916\n",
      "\n",
      "Vocab size: 2048\n",
      "Training accuracy: 0.991, Precison: 0.998, Recall: 0.939, F1 score: 0.967\n",
      "Test accuracy: 0.979, Precison: 0.977, Recall: 0.862, F1 score: 0.916\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "for nf in [8, 16, 32, 64, 128, 256, 512, 1024, 2048]:\n",
    "    print(f\"Vocab size: {nf}\")\n",
    "    \n",
    "    vectorizer = BoWVectorizer(max_features=nf)\n",
    "    vectorizer.fit(train_texts)\n",
    "    \n",
    "    X_train, y_train = vectorizer.transform(train_texts, train_labels)\n",
    "    X_test, y_test = vectorizer.transform(test_texts, test_labels)\n",
    "\n",
    "    # Fit the model to training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_prec = precision_score(y_train, y_train_pred)\n",
    "    train_rec = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec = precision_score(y_test, y_test_pred)\n",
    "    test_rec = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_acc:.3f}, \"\n",
    "          f\"Precison: {train_prec:.3f}, \"\n",
    "          f\"Recall: {train_rec:.3f}, \"\n",
    "          f\"F1 score: {train_f1:.3f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.3f}, \"\n",
    "          f\"Precison: {test_prec:.3f}, \"\n",
    "          f\"Recall: {test_rec:.3f}, \"\n",
    "          f\"F1 score: {test_f1:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 10\n",
      "Training accuracy: 0.891, Precison: 0.764, Recall: 0.274, F1 score: 0.403\n",
      "Test accuracy: 0.880, Precison: 0.704, Recall: 0.131, F1 score: 0.221\n",
      "\n",
      "Vocab size : 20\n",
      "Training accuracy: 0.921, Precison: 0.838, Recall: 0.517, F1 score: 0.639\n",
      "Test accuracy: 0.895, Precison: 0.684, Recall: 0.359, F1 score: 0.471\n",
      "\n",
      "Vocab size : 50\n",
      "Training accuracy: 0.955, Precison: 0.929, Recall: 0.721, F1 score: 0.812\n",
      "Test accuracy: 0.936, Precison: 0.863, Recall: 0.607, F1 score: 0.713\n",
      "\n",
      "Vocab size : 100\n",
      "Training accuracy: 0.968, Precison: 0.946, Recall: 0.807, F1 score: 0.871\n",
      "Test accuracy: 0.946, Precison: 0.938, Recall: 0.628, F1 score: 0.752\n",
      "\n",
      "Vocab size : 200\n",
      "Training accuracy: 0.977, Precison: 0.968, Recall: 0.855, F1 score: 0.908\n",
      "Test accuracy: 0.952, Precison: 0.877, Recall: 0.738, F1 score: 0.801\n",
      "\n",
      "Vocab size : 500\n",
      "Training accuracy: 0.986, Precison: 0.980, Recall: 0.914, F1 score: 0.946\n",
      "Test accuracy: 0.963, Precison: 0.941, Recall: 0.766, F1 score: 0.844\n",
      "\n",
      "Vocab size : 1000\n",
      "Training accuracy: 0.986, Precison: 0.984, Recall: 0.914, F1 score: 0.947\n",
      "Test accuracy: 0.966, Precison: 0.957, Recall: 0.772, F1 score: 0.855\n",
      "\n",
      "Vocab size : 2000\n",
      "Training accuracy: 0.987, Precison: 0.982, Recall: 0.917, F1 score: 0.948\n",
      "Test accuracy: 0.978, Precison: 0.962, Recall: 0.862, F1 score: 0.909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "for vs in [10, 20, 50, 100, 200, 500, 1000, 2000]:    \n",
    "    vectorizer = BiLSTMVectorizer(vocab_size=vs)\n",
    "    \n",
    "    X_train, y_train = vectorizer.transform(train_texts, train_labels)\n",
    "    X_test, y_test = vectorizer.transform(test_texts, test_labels)\n",
    "\n",
    "    # Fit the model to training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_prec = precision_score(y_train, y_train_pred)\n",
    "    train_rec = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec = precision_score(y_test, y_test_pred)\n",
    "    test_rec = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_acc:.3f}, \"\n",
    "          f\"Precison: {train_prec:.3f}, \"\n",
    "          f\"Recall: {train_rec:.3f}, \"\n",
    "          f\"F1 score: {train_f1:.3f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.3f}, \"\n",
    "          f\"Precison: {test_prec:.3f}, \"\n",
    "          f\"Recall: {test_rec:.3f}, \"\n",
    "          f\"F1 score: {test_f1:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Binary Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/sen_imdb/train.csv\")\n",
    "test = pd.read_csv(\"data/sen_imdb/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = list(train.text), list(train.pos)\n",
    "test_texts, test_labels = list(test.text), list(test.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agg method: mean\n",
      "Training accuracy: 0.537, Precison: 0.537, Recall: 0.538, F1 score: 0.538\n",
      "Test accuracy: 0.514, Precison: 0.514, Recall: 0.515, F1 score: 0.514\n",
      "\n",
      "Agg method: first\n",
      "Training accuracy: 0.537, Precison: 0.540, Recall: 0.494, F1 score: 0.516\n",
      "Test accuracy: 0.512, Precison: 0.513, Recall: 0.474, F1 score: 0.493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "for method in [\"mean\", \"first\"]:\n",
    "    print(f\"Agg method: {method}\")\n",
    "    vectorizer = ASCIIVectorizer(max_features=128, char_aggregator=method)\n",
    "    \n",
    "    X_train, y_train = vectorizer.transform(train_texts, train_labels)\n",
    "    X_test, y_test = vectorizer.transform(test_texts, test_labels)\n",
    "    \n",
    "    # Fit the model to training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_prec = precision_score(y_train, y_train_pred)\n",
    "    train_rec = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec = precision_score(y_test, y_test_pred)\n",
    "    test_rec = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_acc:.3f}, \"\n",
    "          f\"Precison: {train_prec:.3f}, \"\n",
    "          f\"Recall: {train_rec:.3f}, \"\n",
    "          f\"F1 score: {train_f1:.3f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.3f}, \"\n",
    "          f\"Precison: {test_prec:.3f}, \"\n",
    "          f\"Recall: {test_rec:.3f}, \"\n",
    "          f\"F1 score: {test_f1:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8\n",
      "Training accuracy: 0.581, Precison: 0.586, Recall: 0.554, F1 score: 0.569\n",
      "Test accuracy: 0.579, Precison: 0.584, Recall: 0.553, F1 score: 0.568\n",
      "\n",
      "Vocab size: 16\n",
      "Training accuracy: 0.612, Precison: 0.610, Recall: 0.619, F1 score: 0.615\n",
      "Test accuracy: 0.612, Precison: 0.610, Recall: 0.619, F1 score: 0.614\n",
      "\n",
      "Vocab size: 32\n",
      "Training accuracy: 0.628, Precison: 0.625, Recall: 0.641, F1 score: 0.633\n",
      "Test accuracy: 0.627, Precison: 0.624, Recall: 0.638, F1 score: 0.631\n",
      "\n",
      "Vocab size: 64\n",
      "Training accuracy: 0.681, Precison: 0.673, Recall: 0.704, F1 score: 0.688\n",
      "Test accuracy: 0.675, Precison: 0.669, Recall: 0.694, F1 score: 0.681\n",
      "\n",
      "Vocab size: 128\n",
      "Training accuracy: 0.756, Precison: 0.747, Recall: 0.773, F1 score: 0.760\n",
      "Test accuracy: 0.746, Precison: 0.739, Recall: 0.761, F1 score: 0.750\n",
      "\n",
      "Vocab size: 256\n",
      "Training accuracy: 0.797, Precison: 0.787, Recall: 0.815, F1 score: 0.800\n",
      "Test accuracy: 0.787, Precison: 0.778, Recall: 0.803, F1 score: 0.790\n",
      "\n",
      "Vocab size: 512\n",
      "Training accuracy: 0.846, Precison: 0.838, Recall: 0.858, F1 score: 0.848\n",
      "Test accuracy: 0.840, Precison: 0.830, Recall: 0.856, F1 score: 0.843\n",
      "\n",
      "Vocab size: 1024\n",
      "Training accuracy: 0.873, Precison: 0.865, Recall: 0.884, F1 score: 0.874\n",
      "Test accuracy: 0.862, Precison: 0.853, Recall: 0.874, F1 score: 0.863\n",
      "\n",
      "Vocab size: 2048\n",
      "Training accuracy: 0.889, Precison: 0.882, Recall: 0.899, F1 score: 0.890\n",
      "Test accuracy: 0.872, Precison: 0.866, Recall: 0.881, F1 score: 0.873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "for nf in [8, 16, 32, 64, 128, 256, 512, 1024, 2048]:\n",
    "    print(f\"Vocab size: {nf}\")\n",
    "    \n",
    "    vectorizer = BoWVectorizer(max_features=nf)\n",
    "    vectorizer.fit(train_texts)\n",
    "    \n",
    "    X_train, y_train = vectorizer.transform(train_texts, train_labels)\n",
    "    X_test, y_test = vectorizer.transform(test_texts, test_labels)\n",
    "\n",
    "    # Fit the model to training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_prec = precision_score(y_train, y_train_pred)\n",
    "    train_rec = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec = precision_score(y_test, y_test_pred)\n",
    "    test_rec = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_acc:.3f}, \"\n",
    "          f\"Precison: {train_prec:.3f}, \"\n",
    "          f\"Recall: {train_rec:.3f}, \"\n",
    "          f\"F1 score: {train_f1:.3f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.3f}, \"\n",
    "          f\"Precison: {test_prec:.3f}, \"\n",
    "          f\"Recall: {test_rec:.3f}, \"\n",
    "          f\"F1 score: {test_f1:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 10\n",
      "Training accuracy: 0.648, Precison: 0.648, Recall: 0.648, F1 score: 0.648\n",
      "Test accuracy: 0.608, Precison: 0.607, Recall: 0.610, F1 score: 0.609\n",
      "\n",
      "Vocab size : 20\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "for vs in [10, 20, 50, 100, 200, 500, 1000, 2000]:    \n",
    "    vectorizer = BiLSTMVectorizer(vocab_size=vs)\n",
    "    \n",
    "    X_train, y_train = vectorizer.transform(train_texts, train_labels)\n",
    "    X_test, y_test = vectorizer.transform(test_texts, test_labels)\n",
    "\n",
    "    # Fit the model to training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_prec = precision_score(y_train, y_train_pred)\n",
    "    train_rec = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec = precision_score(y_test, y_test_pred)\n",
    "    test_rec = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_acc:.3f}, \"\n",
    "          f\"Precison: {train_prec:.3f}, \"\n",
    "          f\"Recall: {train_rec:.3f}, \"\n",
    "          f\"F1 score: {train_f1:.3f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.3f}, \"\n",
    "          f\"Precison: {test_prec:.3f}, \"\n",
    "          f\"Recall: {test_rec:.3f}, \"\n",
    "          f\"F1 score: {test_f1:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
